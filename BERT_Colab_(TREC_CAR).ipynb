{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT for Passage Re-Ranking (TREC-CAR).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SHi-ON/Konigsberg7/blob/master/BERT_Colab_(TREC_CAR).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "5B9HlalBlPtT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**This colab was mainly copied from the original [BERT finetuning tasks in 5 minutes with Cloud TPU](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)**\n",
        "\n",
        "This Colab demonstates using a free Colab Cloud TPU to fine-tune passage re-ranker built on top of pretrained BERT models.\n",
        "\n",
        "Note: You will need a GCP (Google Compute Engine) account and a GCS (Google Cloud Storage) bucket for this Colab to run.\n",
        "\n",
        "Please follow the Google Cloud TPU quickstart for how to create GCP account and GCS bucket. You have $300 free credit to get started with any GCP product. You can learn more about Cloud TPU at https://cloud.google.com/tpu/docs."
      ]
    },
    {
      "metadata": {
        "id": "w5N7IU7zl1UI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Firstly**, we need to set up Colab TPU running environment, verify a TPU device is succesfully connected and upload credentials to TPU for GCS bucket usage.\n"
      ]
    },
    {
      "metadata": {
        "id": "ET8rn1TNhrlc",
        "colab_type": "code",
        "outputId": "181a6db0-0d8f-403b-b78e-426c43a0f603",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU.\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.40.50.146:8470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3ZDNgAEdmRUR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Secondly**, prepare and import BERT modules."
      ]
    },
    {
      "metadata": {
        "id": "USzW4PWnmQqW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "!test -d bert_repo || git clone https://github.com/nyu-dl/dl4marco-bert dl4marco-bert\n",
        "if not 'dl4marco-berto' in sys.path:\n",
        "  sys.path += ['dl4marco-bert']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wVrrnw6Jmj_N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Thirdly**, prepare for training:\n",
        "\n",
        "*  Specify training data.\n",
        "*  Specify BERT pretrained model\n",
        "*  Specify GS bucket, create output directory for model checkpoints and eval results.\n"
      ]
    },
    {
      "metadata": {
        "id": "7UtHASdOmryD",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "INIT_CHECKPOINT = \"gs://neuralresearcher_data/pretrained_models/exp898/model.ckpt-1000000\" #@param {type:\"string\"}\n",
        "print('***** BERT pretrained model: {} *****'.format(INIT_CHECKPOINT))\n",
        "\n",
        "OUTPUT_DIR = 'gs://your_bucket_name_here/output' #@param {type:\"string\"}\n",
        "assert OUTPUT_DIR, 'Must specify an existing GCS bucket name'\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n",
        "\n",
        "# Now we need to specify the input data dir. Should contain the .tfrecord files \n",
        "# and the supporting query-docids mapping files.\n",
        "DATA_DIR = 'gs://your_bucket_name_here' #@param {type:\"string\"}\n",
        "print('***** Data directory: {} *****'.format(DATA_DIR))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "82qY9OMDo2Sg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Now, we can start training/evaluating**"
      ]
    },
    {
      "metadata": {
        "id": "1y_aQMUUGqMj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import metrics\n",
        "import modeling\n",
        "import optimization\n",
        "\n",
        "\n",
        "# Parameters\n",
        "USE_TPU = True\n",
        "DO_TRAIN = True  # Whether to run training.\n",
        "DO_EVAL = True  # Whether to run evaluation.\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "EVAL_BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-6\n",
        "NUM_TRAIN_STEPS = 100000\n",
        "NUM_WARMUP_STEPS = 10000\n",
        "MAX_SEQ_LENGTH = 512\n",
        "SAVE_CHECKPOINTS_STEPS = 1000\n",
        "ITERATIONS_PER_LOOP = 1000\n",
        "NUM_TPU_CORES = 8\n",
        "BERT_CONFIG_FILE = \"gs://cloud-tpu-checkpoints/bert/uncased_L-24_H-1024_A-16/bert_config.json\"\n",
        "TREC_OUTPUT = True  # Whether to write the predictions to a TREC-formatted \"run\" file.\n",
        "MAX_DEV_EXAMPLES = 3000  # Maximum number of dev examples to be evaluated. If None, evaluate all examples in the dev set.\n",
        "NUM_DEV_DOCS = 10  # Number of docs per query in the dev files.\n",
        "MAX_TEST_EXAMPLES = None  # Maximum number of test examples to be evaluated. If None, evaluate all examples in the test set.\n",
        "NUM_TEST_DOCS = 1000  # Number of docs per query in the test files.\n",
        "METRICS_MAP = ['MAP', 'RPrec', 'NDCG', 'MRR', 'MRR@10']\n",
        "FAKE_DOC_ID = \"00000000\"  # Fake doc id used to fill queries with less than num_eval_docs.\n",
        "\n",
        "\n",
        "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
        "                 labels, num_labels, use_one_hot_embeddings):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "  model = modeling.BertModel(\n",
        "      config=bert_config,\n",
        "      is_training=is_training,\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      token_type_ids=segment_ids,\n",
        "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "  output_layer = model.get_pooled_output()\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "    if is_training:\n",
        "      # I.e., 0.1 dropout\n",
        "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "    return (loss, per_example_loss, log_probs)\n",
        "\n",
        "\n",
        "def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n",
        "                     num_train_steps, num_warmup_steps, use_tpu,\n",
        "                     use_one_hot_embeddings):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "    len_gt_titles = features[\"len_gt_titles\"]\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    (total_loss, per_example_loss, log_probs) = create_model(\n",
        "        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
        "        num_labels, use_one_hot_embeddings)\n",
        "\n",
        "    tvars = tf.trainable_variables()\n",
        "\n",
        "    scaffold_fn = None\n",
        "    initialized_variable_names = []\n",
        "    if init_checkpoint:\n",
        "      (assignment_map, initialized_variable_names\n",
        "      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "      if use_tpu:\n",
        "        def tpu_scaffold():\n",
        "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "          return tf.train.Scaffold()\n",
        "\n",
        "        scaffold_fn = tpu_scaffold\n",
        "      else:\n",
        "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "    tf.logging.info(\"**** Trainable Variables ****\")\n",
        "    for var in tvars:\n",
        "      init_string = \"\"\n",
        "      if var.name in initialized_variable_names:\n",
        "        init_string = \", *INIT_FROM_CKPT*\"\n",
        "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
        "                      init_string)\n",
        "\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "\n",
        "      train_op = optimization.create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          train_op=train_op,\n",
        "          scaffold_fn=scaffold_fn)\n",
        "\n",
        "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          predictions={\n",
        "              \"log_probs\": log_probs,\n",
        "              \"label_ids\": label_ids,\n",
        "              \"len_gt_titles\": len_gt_titles,\n",
        "          },\n",
        "          scaffold_fn=scaffold_fn)\n",
        "\n",
        "    else:\n",
        "      raise ValueError(\n",
        "          \"Only TRAIN and PREDICT modes are supported: %s\" % (mode))\n",
        "\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn\n",
        "\n",
        "\n",
        "def input_fn_builder(dataset_path, seq_length, is_training,\n",
        "                     max_eval_examples=None):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "\n",
        "    batch_size = params[\"batch_size\"]\n",
        "    output_buffer_size = batch_size * 1000\n",
        "\n",
        "    def extract_fn(data_record):\n",
        "      features = {\n",
        "          \"query_ids\": tf.FixedLenSequenceFeature(\n",
        "              [], tf.int64, allow_missing=True),\n",
        "          \"doc_ids\": tf.FixedLenSequenceFeature(\n",
        "              [], tf.int64, allow_missing=True),\n",
        "          \"label\": tf.FixedLenFeature([], tf.int64),\n",
        "          \"len_gt_titles\": tf.FixedLenFeature([], tf.int64),\n",
        "      }\n",
        "      sample = tf.parse_single_example(data_record, features)\n",
        "\n",
        "      query_ids = tf.cast(sample[\"query_ids\"], tf.int32)\n",
        "      doc_ids = tf.cast(sample[\"doc_ids\"], tf.int32)\n",
        "      label_ids = tf.cast(sample[\"label\"], tf.int32)\n",
        "      len_gt_titles = tf.cast(sample[\"len_gt_titles\"], tf.int32)\n",
        "      \n",
        "      input_ids = tf.concat((query_ids, doc_ids), 0)\n",
        "\n",
        "      query_segment_id = tf.zeros_like(query_ids)\n",
        "      doc_segment_id = tf.ones_like(doc_ids)\n",
        "      segment_ids = tf.concat((query_segment_id, doc_segment_id), 0)\n",
        "\n",
        "      input_mask = tf.ones_like(input_ids)\n",
        "\n",
        "      features = {\n",
        "          \"input_ids\": input_ids,\n",
        "          \"segment_ids\": segment_ids,\n",
        "          \"input_mask\": input_mask,\n",
        "          \"label_ids\": label_ids,\n",
        "          \"len_gt_titles\": len_gt_titles,\n",
        "      }\n",
        "      return features\n",
        "\n",
        "    dataset = tf.data.TFRecordDataset([dataset_path])\n",
        "    dataset = dataset.map(\n",
        "        extract_fn, num_parallel_calls=4).prefetch(output_buffer_size)\n",
        "\n",
        "    if is_training:\n",
        "      dataset = dataset.repeat()\n",
        "      dataset = dataset.shuffle(buffer_size=1000)\n",
        "    else:\n",
        "      if max_eval_examples:\n",
        "        dataset = dataset.take(max_eval_examples)\n",
        "\n",
        "    dataset = dataset.padded_batch(\n",
        "        batch_size=batch_size,\n",
        "        padded_shapes={\n",
        "            \"input_ids\": [seq_length],\n",
        "            \"segment_ids\": [seq_length],\n",
        "            \"input_mask\": [seq_length],\n",
        "            \"label_ids\": [],\n",
        "            \"len_gt_titles\": [],\n",
        "        },\n",
        "        padding_values={\n",
        "            \"input_ids\": 0,\n",
        "            \"segment_ids\": 0,\n",
        "            \"input_mask\": 0,\n",
        "            \"label_ids\": 0,\n",
        "            \"len_gt_titles\": 0,\n",
        "        },\n",
        "        drop_remainder=True)\n",
        "\n",
        "    return dataset\n",
        "  return input_fn\n",
        "\n",
        "\n",
        "def main(_):\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  \n",
        "  if not DO_TRAIN and not DO_EVAL:\n",
        "    raise ValueError(\"At least one of `DO_TRAIN` or `DO_EVAL` must be True.\")\n",
        "\n",
        "  bert_config = modeling.BertConfig.from_json_file(BERT_CONFIG_FILE)\n",
        "\n",
        "  if MAX_SEQ_LENGTH > bert_config.max_position_embeddings:\n",
        "    raise ValueError(\n",
        "        \"Cannot use sequence length %d because the BERT model \"\n",
        "        \"was only trained up to sequence length %d\" %\n",
        "        (MAX_SEQ_LENGTH, bert_config.max_position_embeddings))\n",
        "\n",
        "  tpu_cluster_resolver = None\n",
        "  if USE_TPU:\n",
        "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
        "        TPU_ADDRESS)\n",
        "\n",
        "  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
        "  run_config = tf.contrib.tpu.RunConfig(\n",
        "      cluster=tpu_cluster_resolver,\n",
        "      model_dir=OUTPUT_DIR,\n",
        "      save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "          iterations_per_loop=ITERATIONS_PER_LOOP,\n",
        "          num_shards=NUM_TPU_CORES,\n",
        "          per_host_input_for_training=is_per_host))\n",
        "\n",
        "  model_fn = model_fn_builder(\n",
        "      bert_config=bert_config,\n",
        "      num_labels=2,\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      num_train_steps=NUM_TRAIN_STEPS,\n",
        "      num_warmup_steps=NUM_WARMUP_STEPS,\n",
        "      use_tpu=USE_TPU,\n",
        "      use_one_hot_embeddings=USE_TPU)\n",
        "\n",
        "  # If TPU is not available, this will fall back to normal Estimator on CPU\n",
        "  # or GPU.\n",
        "  estimator = tf.contrib.tpu.TPUEstimator(\n",
        "      use_tpu=USE_TPU,\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      train_batch_size=TRAIN_BATCH_SIZE,\n",
        "      eval_batch_size=EVAL_BATCH_SIZE,\n",
        "      predict_batch_size=EVAL_BATCH_SIZE)\n",
        "\n",
        "  if DO_TRAIN:\n",
        "    tf.logging.info(\"***** Running training *****\")\n",
        "    tf.logging.info(\"  Batch size = %d\", TRAIN_BATCH_SIZE)\n",
        "    tf.logging.info(\"  Num steps = %d\", NUM_TRAIN_STEPS)\n",
        "    train_input_fn = input_fn_builder(\n",
        "        dataset_path=os.path.join(DATA_DIR, \"dataset_train.tf\"),\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=True)\n",
        "    estimator.train(input_fn=train_input_fn,\n",
        "                    max_steps=NUM_TRAIN_STEPS)\n",
        "    tf.logging.info(\"Done Training!\")\n",
        "\n",
        "  if DO_EVAL:\n",
        "    for set_name in [\"test\"]:\n",
        "      tf.logging.info(\"***** Running evaluation on the {} set*****\".format(\n",
        "          set_name))\n",
        "      tf.logging.info(\"  Batch size = %d\", EVAL_BATCH_SIZE)\n",
        "      \n",
        "      max_eval_examples = None\n",
        "      \n",
        "      if set_name == \"dev\":\n",
        "        num_eval_docs = NUM_DEV_DOCS\n",
        "        if MAX_DEV_EXAMPLES:\n",
        "          max_eval_examples = MAX_DEV_EXAMPLES * NUM_DEV_DOCS\n",
        "          \n",
        "      elif set_name == \"test\":\n",
        "        num_eval_docs = NUM_TEST_DOCS\n",
        "        if MAX_TEST_EXAMPLES:\n",
        "          max_eval_examples = MAX_TEST_EXAMPLES * NUM_TEST_DOCS\n",
        "          \n",
        "      eval_input_fn = input_fn_builder(\n",
        "          dataset_path=os.path.join(DATA_DIR, \"dataset_\" + set_name + \".tf\"),\n",
        "          seq_length=MAX_SEQ_LENGTH,\n",
        "          is_training=False,\n",
        "          max_eval_examples=max_eval_examples)\n",
        "\n",
        "      if TREC_OUTPUT:\n",
        "        trec_file = tf.gfile.Open(\n",
        "            os.path.join(\n",
        "                OUTPUT_DIR, \"bert_predictions_\" + set_name + \".run\"), \"w\")\n",
        "        query_docids_map = []\n",
        "        docs_per_query = 0  # Counter of docs per query\n",
        "        with tf.gfile.Open(\n",
        "            os.path.join(DATA_DIR, set_name + \".run\")) as ref_file:\n",
        "          for line in ref_file:\n",
        "            query, _, doc_id, _, _, _ = line.strip().split(\" \")\n",
        "            \n",
        "            # We add fake docs so the number of docs per query is always\n",
        "            # num_eval_docs.\n",
        "            if len(query_docids_map) > 0:\n",
        "              if query != last_query:\n",
        "                if docs_per_query < num_eval_docs:\n",
        "                  fake_pairs = (num_eval_docs - docs_per_query) * [\n",
        "                      (last_query, FAKE_DOC_ID)]\n",
        "                  query_docids_map.extend(fake_pairs)\n",
        "                docs_per_query = 0\n",
        "\n",
        "            query_docids_map.append((query, doc_id))\n",
        "            last_query = query\n",
        "            docs_per_query += 1\n",
        "            \n",
        "      # ***IMPORTANT NOTE***\n",
        "      # The logging output produced by the feed queues during evaluation is very \n",
        "      # large (~14M lines for the dev set), which causes the tab to crash if you\n",
        "      # don't have enough memory on your local machine. We suppress this \n",
        "      # frequent logging by setting the verbosity to WARN during the evaluation\n",
        "      # phase.\n",
        "      tf.logging.set_verbosity(tf.logging.WARN)\n",
        "      \n",
        "      result = estimator.predict(input_fn=eval_input_fn,\n",
        "                                 yield_single_examples=True)\n",
        "      start_time = time.time()\n",
        "      results = []\n",
        "      all_metrics = np.zeros(len(METRICS_MAP))\n",
        "      example_idx = 0\n",
        "      total_count = 0\n",
        "      for item in result:\n",
        "        results.append(\n",
        "            (item[\"log_probs\"], item[\"label_ids\"], item[\"len_gt_titles\"]))\n",
        "\n",
        "        if len(results) == num_eval_docs:\n",
        "\n",
        "          log_probs, labels, len_gt_titles = zip(*results)\n",
        "          log_probs = np.stack(log_probs).reshape(-1, 2)\n",
        "          labels = np.stack(labels)\n",
        "          len_gt_titles = np.stack(len_gt_titles)\n",
        "          assert len(set(list(len_gt_titles))) == 1, (\n",
        "              \"all ground truth lengths must be the same for a given query.\")   \n",
        "          \n",
        "          scores = log_probs[:, 1]\n",
        "          pred_docs = scores.argsort()[::-1]\n",
        "          \n",
        "          gt = set(list(np.where(labels > 0)[0]))\n",
        "\n",
        "          # Metrics like NDCG and MAP require the total number of relevant docs.\n",
        "          # The code below adds missing number of relevant docs to gt so the \n",
        "          # metrics are the same as if we had used all ground-truths.\n",
        "          # The extra_gts have all negative ids so they don't interfere with the\n",
        "          # predicted ids, which are all equal or greater than zero.\n",
        "          extra_gts = list(-(np.arange(max(0, len_gt_titles[0] - len(gt))) + 1))\n",
        "          gt.update(extra_gts)\n",
        "          \n",
        "          all_metrics += metrics.metrics(\n",
        "              gt=gt, pred=pred_docs, metrics_map=METRICS_MAP)\n",
        "\n",
        "          if TREC_OUTPUT:\n",
        "            start_idx = example_idx * num_eval_docs\n",
        "            end_idx = (example_idx + 1) * num_eval_docs\n",
        "            queries, doc_ids = zip(*query_docids_map[start_idx:end_idx])\n",
        "            assert len(set(queries)) == 1, \"Queries must be all the same.\"\n",
        "            query = queries[0]\n",
        "            rank = 1\n",
        "            for doc_idx in pred_docs:\n",
        "              doc_id = doc_ids[doc_idx]\n",
        "              score = scores[doc_idx]\n",
        "              # Skip fake docs, as they are only used to ensure that each query\n",
        "              # has 1000 docs.\n",
        "              if doc_id != FAKE_DOC_ID:\n",
        "                output_line = \" \".join(\n",
        "                    (query, \"Q0\", doc_id, str(rank), str(score), \"BERT\"))\n",
        "                trec_file.write(output_line + \"\\n\")\n",
        "                rank += 1\n",
        "\n",
        "          example_idx += 1\n",
        "          results = []\n",
        "          \n",
        "        total_count += 1\n",
        "        \n",
        "        if total_count % 10000 == 0:\n",
        "          tf.logging.warn(\"Read {} examples in {} secs. Metrics so far:\".format(\n",
        "              total_count, int(time.time() - start_time)))\n",
        "          tf.logging.warn(\"  \".join(METRICS_MAP))\n",
        "          tf.logging.warn(all_metrics / example_idx)\n",
        "          \n",
        "      # Once the feed queues are finished, we can set the verbosity back to \n",
        "      # INFO.\n",
        "      tf.logging.set_verbosity(tf.logging.INFO)\n",
        "      \n",
        "      if TREC_OUTPUT:\n",
        "        trec_file.close()\n",
        "\n",
        "      all_metrics /= example_idx\n",
        "\n",
        "      tf.logging.info(\"Eval {}:\".format(set_name))\n",
        "      tf.logging.info(\"  \".join(METRICS_MAP))\n",
        "      tf.logging.info(all_metrics)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  tf.app.run()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}