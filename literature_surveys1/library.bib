Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop


@article{Kuzi,
abstract = {We present a suite of query expansion methods that are based on word embeddings. Using Word2Vec's CBOW embedding approach, applied over the entire corpus on which search is performed, we select terms that are semantically related to the query. Our methods either use the terms to expand the original query or integrate them with the effective pseudo-feedback-based relevance model. In the former case, retrieval performance is significantly better than that of using only the query, and in the latter case the performance is significantly better than that of the relevance model.},
author = {Kuzi, Saar and Shtok, Anna and Kurland, Oren},
doi = {10.1145/2983323.2983876},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuzi, Shtok, Kurland - Unknown - Query Expansion Using Word Embeddings.pdf:pdf},
isbn = {9781450340731},
title = {{Query Expansion Using Word Embeddings}},
url = {http://dx.doi.org/10.1145/2983323.2983876}
}
@article{Roy2016,
abstract = {In this paper a framework for Automatic Query Expansion (AQE) is proposed using distributed neural language model word2vec. Using semantic and contextual relation in a distributed and unsupervised framework, word2vec learns a low dimensional embedding for each vocabulary entry. Using such a framework, we devise a query expansion technique, where related terms to a query are obtained by K-nearest neighbor approach. We explore the performance of the AQE methods, with and without feedback query expansion, and a variant of simple K-nearest neighbor in the proposed framework. Experiments on standard TREC ad-hoc data (Disk 4, 5 with query sets 301-450, 601-700) and web data (WT10G data with query set 451-550) shows significant improvement over standard term-overlapping based retrieval methods. However the proposed method fails to achieve comparable performance with statistical co-occurrence based feedback method such as RM3. We have also found that the word2vec based query expansion methods perform similarly with and without any feedback information.},
archivePrefix = {arXiv},
arxivId = {1606.07608},
author = {Roy, Dwaipayan and Paul, Debjyoti and Mitra, Mandar and Garain, Utpal},
eprint = {1606.07608},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Roy et al. - 2016 - Using Word Embeddings for Automatic Query Expansion.pdf:pdf},
month = {jun},
title = {{Using Word Embeddings for Automatic Query Expansion}},
url = {http://arxiv.org/abs/1606.07608},
year = {2016}
}

@article{Kuzi,
abstract = {We present a suite of query expansion methods that are based on word embeddings. Using Word2Vec's CBOW embedding approach, applied over the entire corpus on which search is performed, we select terms that are semantically related to the query. Our methods either use the terms to expand the original query or integrate them with the effective pseudo-feedback-based relevance model. In the former case, retrieval performance is significantly better than that of using only the query, and in the latter case the performance is significantly better than that of the relevance model.},
author = {Kuzi, Saar and Shtok, Anna and Kurland, Oren},
doi = {10.1145/2983323.2983876},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuzi, Shtok, Kurland - Unknown - Query Expansion Using Word Embeddings.pdf:pdf},
isbn = {9781450340731},
title = {{Query Expansion Using Word Embeddings}},
url = {http://dx.doi.org/10.1145/2983323.2983876}
}

@article{Nogueira2017,
abstract = {Search engines play an important role in our everyday lives by assisting us in finding the information we need. When we input a complex query, however, results are often far from satisfactory. In this work, we introduce a query reformulation system based on a neural network that rewrites a query to maximize the number of relevant documents returned. We train this neural network with reinforcement learning. The actions correspond to selecting terms to build a reformulated query, and the reward is the document recall. We evaluate our approach on three datasets against strong baselines and show a relative improvement of 5-20{\%} in terms of recall. Furthermore, we present a simple method to estimate a conservative upper-bound performance of a model in a particular environment and verify that there is still large room for improvements.},
archivePrefix = {arXiv},
arxivId = {1704.04572},
author = {Nogueira, Rodrigo and Cho, Kyunghyun},
eprint = {1704.04572},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nogueira, Cho - 2017 - Task-Oriented Query Reformulation with Reinforcement Learning.pdf:pdf},
month = {apr},
title = {{Task-Oriented Query Reformulation with Reinforcement Learning}},
url = {http://arxiv.org/abs/1704.04572},
year = {2017}
}

@inproceedings{Lavrenko2001,
address = {New York, New York, USA},
author = {Lavrenko, Victor and Croft, W. Bruce},
booktitle = {Proc. 24th Annu. Int. ACM SIGIR Conf. Res. Dev. Inf. Retr.  - SIGIR '01},
doi = {10.1145/383952.383972},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lavrenko, Croft - 2001 - Relevance based language models.pdf:pdf},
isbn = {1581133316},
pages = {120--127},
publisher = {ACM Press},
title = {{Relevance based language models}},
url = {http://portal.acm.org/citation.cfm?doid=383952.383972},
year = {2001}
}

@article{Narasimhan2016,
abstract = {Most successful information extraction systems operate with access to a large collection of documents. In this work, we explore the task of acquiring and incorporating external evidence to improve extraction accuracy in domains where the amount of training data is scarce. This process entails issuing search queries, extraction from new sources and reconciliation of extracted values, which are repeated until sufficient evidence is collected. We approach the problem using a reinforcement learning framework where our model learns to select optimal actions based on contextual information. We employ a deep Q-network, trained to optimize a reward function that reflects extraction accuracy while penalizing extra effort. Our experiments on two databases -- of shooting incidents, and food adulteration cases -- demonstrate that our system significantly outperforms traditional extractors and a competitive meta-classifier baseline.},
archivePrefix = {arXiv},
arxivId = {1603.07954},
author = {Narasimhan, Karthik and Yala, Adam and Barzilay, Regina},
eprint = {1603.07954},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Narasimhan, Yala, Barzilay - 2016 - Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning.pdf:pdf},
month = {mar},
title = {{Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning}},
url = {http://arxiv.org/abs/1603.07954},
year = {2016}
}

@article{Devlin2018,
abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4{\%} (7.6{\%} absolute improvement), MultiNLI accuracy to 86.7 (5.6{\%} absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5{\%} absolute improvement), outperforming human performance by 2.0{\%}.},
archivePrefix = {arXiv},
arxivId = {1810.04805},
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
eprint = {1810.04805},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Devlin et al. - 2018 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:pdf},
month = {oct},
title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
url = {http://arxiv.org/abs/1810.04805},
year = {2018}
}

@article{Peters2018,
abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
archivePrefix = {arXiv},
arxivId = {1802.05365},
author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
eprint = {1802.05365},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Peters et al. - 2018 - Deep contextualized word representations.pdf:pdf},
month = {feb},
title = {{Deep contextualized word representations}},
url = {http://arxiv.org/abs/1802.05365},
year = {2018}
}


@article{Radford,
author = {Radford, A and Narasimhan, K and https://s3-us-west-2 {\ldots}, T Salimans - URL and undefined 2018},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Radford et al. - Unknown - Improving language understanding by generative pre-training.pdf:pdf},
journal = {cs.ubc.ca},
title = {{Improving language understanding by generative pre-training}},
url = {https://www.cs.ubc.ca/{~}amuham01/LING530/papers/radford2018improving.pdf}
}

@techreport{Dietz,
abstract = {This presentation will provide an overview of the challenges of high power proton accelerators such as SNS, J-PARC, etc., and what we have learned from recent experiences. Beam loss mechanisms and methods to mitigate beam loss will also be discussed.},
author = {Dietz, Laura and Verma, Manisha and Radlinski, Filip and Craswell, Nick},
booktitle = {Trec},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dietz et al. - Unknown - TREC Complex Answer Retrieval Overview.pdf:pdf},
pages = {1--13},
title = {{TREC Complex Answer Retrieval Overview}},
url = {http://trec-car.cs.unh.edu},
year = {2017}
}

@article{Nogueira2019,
abstract = {Recently, neural models pretrained on a language modeling task, such as ELMo (Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018), have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference. In this paper, we describe a simple re-implementation of BERT for query-based passage re-ranking. Our system is the state of the art on the TREC-CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task, outperforming the previous state of the art by 27{\%} (relative) in MRR@10. The code to reproduce our results is available at https://github.com/nyu-dl/dl4marco-bert},
archivePrefix = {arXiv},
arxivId = {1901.04085},
author = {Nogueira, Rodrigo and Cho, Kyunghyun},
eprint = {1901.04085},
file = {:home/shi-on/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nogueira, Cho - 2019 - Passage Re-ranking with BERT(2).pdf:pdf},
month = {jan},
title = {{Passage Re-ranking with BERT}},
url = {http://arxiv.org/abs/1901.04085},
year = {2019}
}
