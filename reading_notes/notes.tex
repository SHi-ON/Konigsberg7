\documentclass[letterpaper,12pt]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

%
% For alternative styles, see the biblatex manual:
% http://mirrors.ctan.org/macros/latex/contrib/biblatex/doc/biblatex.pdf
%
% The 'verbose' family of styles produces full citations in footnotes, 
% with and a variety of options for ibidem abbreviations.
%
\usepackage{graphicx}
\usepackage{csquotes}
\usepackage[style=verbose-ibid,backend=bibtex]{biblatex}
\bibliography{sample}

\usepackage{lipsum} % for dummy text

\title{Data Science with Knowledge Graph \\{\large Reading Notes}}
\author{Shawn Azdam}

\date{\today}

\begin{document}
\maketitle

\section{Complex Answer Retrieval}
According to the two mentioned papers, we clearly get a whole picture of how the mentioned TREC's track progressed in a span of two years. Based on pragmatic examples, the authors have tried to illustrate different CAR tasks. Compared to the previous year, an improvement in the data set has been reported from a volume and also a quality point of view so they went through a more sophisticated process of data set preparation. As a measure of evaluation, a dual process of assessment of submissions to CAR track has proposed which comprised of the automatic and the manual ground truth signals. Due to a major alteration in paragraphs available in Wiki-18 in a comparison with Wiki-16, the former automatic assessment procedure utilized in Y1 has been rendered obsolete. By taking a side-by-side collation view of the variety of submission's results, we could noticeably observe that the overall result has improved. This improvement happened in terms of measures such as RPrec, NDCG, MAP, etc. in both declared tasks.

\section{Graph Walking}
The paper proposes computing a set of PageRank vectors as it contrasts with a single PageRank vector. The author also adds that adding contextual orientation by a set of representative topics selected to cover the significance of each topic. This query-specific approach -as it has been showed- can result in higher accuracy in ranking compared to the generic vector of PageRank (PR). After pointing out the three past works done related to HITS algorithms, He quickly describes PR advantages over HITS wrapped up in two fields, query-time cost efficacy, and localized link spam prevention. Later two scenarios are discussed, first is so-called conventional querying the search engine and then \textit{search in context}. In order to frame the this paper's approach, I have reached to two distinct modes. In \textit{offline mode}, 16 biased topic-sensitive PR vectors are generated. At \textit{query time}, the similarity of the query to each vector is generated. By utilizing a linear combination of the query-specific weighted topic-sensitive vectors to build up a composite PageRank score, a higher accuracy in getting significantly related results to the query (if available, query context) will be achievable. Two phases have important roles when it comes to the performance of this method, first offline generating biased PageRank vectors utilizing a stack of basis topics during the pre-processing stage of the Web crawl and then query-sensitive PageRank computation which has been done during query time starting with computing class probabilities for each of 16 top-level Open Directory Project (ODP) classes using a unigram language model. The author then illustrated the results which have two scopes. By studying the pairwise topically-biased results we realize that ODP-biasing has tangibly improved rankings which also makes sense intuitively. Figure 1 clearly depicts improvement over precision and along with the manual selected ranking by users show that query-sensitive scores desirably performs better. In the end, the paper points out flexibility, transparency, privacy, and efficiency in the source of the search context.

\section{Entity Linking}
In this survey paper, I'm trying to frame the whole picture in form of a bullet list. Authors in this paper tried to present a a comprehensive survey for entity linking. Specifically, they have surveyed the main approaches utilized in the three modules of entity linking systems (i.e., Candidate Entity Generation, Candidate Entity Ranking, and Unlinkable Mention Prediction), and also introduced other critical aspects of entity linking such as applications,features, and evaluation. One can define Entity linking as a task of mapping named entity mention $\m \in M$ available in an unstructured text (without attributes) to corresponding entity $e \in E$ in a knowledge base (with attributes). Three main stages in an entity linking systems are candidate \textit{entity generation}, \textit{candidate entity ranking}, and \textit{unlinkable mention prediction}, which could be applicable to several tasks such as information extraction, information retrieval, content analysis, question answering, and finally knowledge base population.

\begin{itemize}
    \item Candidate Entity Generation
\begin{itemize}
    \item Name dictionary based techniques: the structure of Wikipedia provides a
set of useful features for generating candidate entities,
such as entity pages, redirect pages, disambiguation
pages, bold phrases from the first paragraphs, and
hyperlinks in Wikipedia articles. These entity linking
systems leverage different combinations of these
features to build an offline name dictionary $D$
between various names and their possible mapping
entities, and exploit this constructed name dictionary
$D$ to generate candidate entities. Query click log and web documents to find entity synonyms are also helpful in name dict construction.
    \item Surface form expansion from the local document: identifying other possible expanded variations (such as
the full name) from the associated document where
the entity mention appears which could be discussed in form of heuristic based methods or supervised methods (finds more sophisticated acronyms).
    \item Methods based on search engines: identifying candidates from web search engines such as Google or Wikipedia search engine.
\end{itemize}

\item Candidate Entity Ranking
this is a key component for the entity linking system. We can broadly divide these candidate entity ranking methods into two categories, Supervised and Unsupervised ranking methods. One can categorise methods in three other groups based on documents dependency such as \textit{independent}, \textit{collective}, and \textit{collaborative}\textit{ ranking methods}.
\begin{itemize}
    \item Candidate entity ranking features: two categories are discussable here based on context dependency. Context independent features just rely on the surface form of the entity mention and the knowledge about the candidate entity, and are not related to the context where the entity mention appears. Context-dependent features are based on the context where the entity mention appears. Two forms to represent the textual context could be found like Bag of words and concept vector. Here there is an interesting discussed concept  coherence between mapping entities which could be defined between two Wikipedia entities $v_1$ and $v_2$ as follows:
    \begin{equation}
        \operatorname { Coh } _ { G } \left( u _ { 1 } , u _ { 2 } \right) = 1 - \frac { \log \left( \max \left( \left| U _ { 1 } \right| , \left| U _ { 2 } \right| \right) \right) - \log \left( \left| U _ { 1 } \cap U _ { 2 } \right| \right) } { \log ( | W P | ) - \log \left( \min \left( \left| U _ { 1 } \right| , \left| U _ { 2 } \right| \right) \right) }
    \end{equation}
    \item Supervised entity ranking: these methods learn to assign proper mapping entity to each entity mention.
        \begin{itemize}
            \item Binary classification methods: binary classifier to decide if the entity mention refers to the candidate entity.
            \item Learning to rank methods: automatically construct a ranking model from training data. Most of such systems uses SVM methods.
            \item Probabilistic methods: modeling in combination with pairwise document-level topical coherence of candidate entities using a probabilistic graphical model.
            \item Graph based approaches: an inference algorithm to jointly infer mapping entities of all entity mentions in the same document based on a graph based representation \textit{Referent Graph}.
            \item Model combination: combining different learning algorithms to obtain better predictive performance.
            \item Training data generation: manually created data set containing of thousands of labeled entities.
        \end{itemize}
    \item Unsupervised ranking methods:
        \begin{itemize}
            \item VSM based methods: using vector space model to represent similarity between entity mentions and candidate entity.
            \item IR based methods: indexing each candidate as a document and generating query for each mention.
        \end{itemize}
\end{itemize}

\item Unlinkable Mention Prediction
In practice, some entity mention does not have its corresponding record in a knowledge base. Therefore, they have to deal with the problem of predicting unlinkable mentions.
\end{itemize}

Name variation and entity ambiguity are two challenges which an entity linking task may face. Other similar available task in the literature to entity linking are as follows:
\begin{itemize}
    \item Entity co-referencing: without presence of a knowledge base.
    \item Word sense disambiguation: identifying sense of a word instead of a named entity.
    \item Record linkage: matching records from multiple sources (rather than one knowledge base) which refer to the same entity.
\end{itemize}

For the sake of evaluation, some measures like precision, recall, F1-measure, and accuracy are informative to calculate.
Although so many methods are discussed here in this paper, but there are still some unaddressed points in the literature which they propose as a future pathway.

\section{Knowledge Graph}
Knowledge graphs model information in the form of entities (\textit{subject}, \textit{object}) and relationships between them (\textit{predicate}) (In form of SPO triples in this paper). Statistical relational learning (SRL) has been used to identify and make relationship between entities.There are two main classes of SRL techniques: those that capture the correlation between the nodes/edges using latent variables, and those that capture the correlation directly using statistical models based on the observable properties of the graph. KGs can provide type hierarchy and type constraints. Different paradigms for the interpretation of non-existing triples are closed and open world assumptions (\textit{CWA}, \textit{OWA}). Knowledge base construction method can come in 4 categories: curated, collaborative, automated semi-structured, and automated unstructured. Despite the accuracy in semi-structured KGs coverage is another important issue of them. KGs could be categorize in either being \textit{schema-based} or \textit{schema-free} while the majority of KGs are from the former one. Common KG tasks are as follows: Link prediction, Entity resolution, and Link-based clustering. As an application to TREC CAR task, one may propose to use KG to enhance our search results in TREC CAR. In this way, we turned the existing query(text)-based search into a semantically aware one.

\section{Text Interfaces to SPARQL}

The paper main contribution is a semantic parser which can learn from question-answer pairs (denotations) rather than manually annotated logical forms. Adopting this approach collects a large number of candidate logical predicates to answer the questions. Two techniques are implemented to resolve the aforementioned issue, first by aligning natural language phrases (from the large text corpus) with logical predicates (from the knowledge base) which happen if they co-occur with several entities and second by utilizing a bridging operation based on adjacent predicates. Bridging has been introduced along with alignment because although alignment can cover (in terms of semantic realization) many logical predicates, it is unreliable in case of weakly expressed or vague predicates. They have paid attention in building the model to a set of special features along with operations (intersection, join, bridging) to generate predictions. The body of work is considerable because it introduces ways to tackle the supervised learning of semantics models with direct interaction with the world. In retrospect, we talked about entity linking in the class which is an obvious element in feature alignment (constructing bipartite over phrases and predicates). Talking from TREC CAR point of view, inspired by this paper, a robust semantic parser can analyze the query and address parts of the query with some logical forms (e.g. mapping to "\textit{where}" and "\textit{how many}" to \textbf{Type.Location} and \textbf{Count}). Later combined with an entity linking system it can possibly increase the precision as it can return more relevant documents by detecting the semantic behind a specific query.

\section{Entity-Aspect Linking}
Entity-aspect linking (EAL), in a nutshell, is defined as linking an entity mention to one aspect from an already defined set of aspects which covers the addressed topic. Aspects of an entity, in this paper, are mostly related sections of the Wikipedia page related to that entity. Besides that, the authors have mentioned two other ways of aspect detection: extracting information from large-scale query logs and entity profiling to generate salient content about an entity from a corpus. As the outcome, entities accompany semantically enriched information which yields a refined entity link between mentions and entities in a collection.EAL method consists of two parts: 1) aspect representation which could be done by whether a similarity comparison between the mention in the context and header (or content) of the Wikipedia page or a comparison between the mentioned entities in the context and the content. 2) aspects ranking by calculating feature-vectors in form of either symbolic or vector representation of each entity. Compared to previous works, this paper has examined a wide contextual variety which makes this aspect-linking method applicable to a different length of contexts. For instance, the test of Sentence accommodates EAL for shorter contexts such as tweets or other social media messages while the test of Section affords EAL testing for longer contexts such as news articles.


\section{Entity Ranking}
This paper proposed Fielded Sequential Dependence Model,
a novel retrieval model, which incorporates term dependencies into structured document retrieval, and a two-stage algorithm to directly optimize the parameters of this model with respect to the target retrieval metric. The demand for efficient access to knowledge graphs gives rise to the task of Ad-hoc Entity Retrieval from the Web of Data (ERWD). The goal of ERWD is to return an entity or a list of entities based on their unconstrained keyword descriptions, as opposed to the traditional information retrieval task, in which search systems return documents in response to keyword queries. This task is focused on retrieval
from knowledge bases, as opposed to utilization of knowledge
bases in retrieval. Major open source knowledge bases typically adopt Resource Description Framework (RDF) data model and are published as part of the Linked Open Data (LOD) cloud commonly referred to as the Web of Data. Individual RDF datasets in the Web of Data can be considered as massive graphs, in which the nodes are the entities (resources) and the edges are semantic relations between the entities. Each resource describes an object in the Web of Data, which can either be a real entity or an abstract concept (special relativity). The relations between the entities are represented as subject-predicate-object triples. Moreover, authors a five-field entity representation scheme in which the first two fields are the properties of the entities themselves. Other fields capture information from different entities that are related to the given entity. They demonstrated that having different field weighting schemes for unigrams and bigrams is effective for different types of queries in ad-hoc entity retrieval scenario. Experimental
evaluation of FSDM on a standard publicly available benchmark showed that it consistently and, in most cases, statistically significantly outperforms state-of-the-art structured and unstructured retrieval models for ERWD.

\section{Latent Entity Space}
This paper proposes Latent Entity Space (LES) a retrieval approach which models document relevance using entity profiles to delineate semantic content of queries and documents. In LES as an entity-based space, each entity corresponds to one dimension, representing one semantic relevance aspect. In the paper, each dimension is represented with the profile of the corresponding entity. Information from document collection and knowledge bases are used to construct an entity profile. 

In comparison to concept-based IR approaches, LES does not modify the representation of queries and documents. LES leverages entity profiles as a bridge to measure semantic relevance between queries and documents. This approach is also computationally efficient because the entity model could be obtained offline. Instead of performing query expansion, LES leverages the information from knowledge bases to build entity profiles.

Experimental results demonstrated that LES is capable of capturing additional semantic content missed by existing models including the Relevance Model and Latent Concept Expansion.

\section{Using Relations for IR}

\end{document}